# <div style="text-align: center;"><img src="./figure/logo.png" width="60" height="60" /> </div>
# Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence  

## ğŸ”¥ Updates

ğŸš€ **November 17, 2025** â€” We release the [Conan-91k](https://huggingface.co/RUBBISHLIKE/Conan-91k) dataset!

ğŸš€ **October 25, 2025** â€” We release the training framework of [Conan](./ms-swift/)!

ğŸš€ **October 23, 2025** â€” We release the [paper](https://arxiv.org/abs/2510.20470)!

ğŸš€ **October 20, 2025** â€” We are excited to release [Conan-7B](https://huggingface.co/RUBBISHLIKE/Conan-7B) and its accompanying evaluation toolkit, [Conan-Eval](./Conan-Eval/)!

ğŸš€ **September 30, 2025** â€” [Conan-SFT-7B](https://huggingface.co/RUBBISHLIKE/Conan-7B-SFT) has officially landed on Hugging Face!

## Introduction
Conan is an innovative Multimodal Large Language Model (MLLM) designed with advanced reasoning capabilities inspired by a detective's investigative process. It excels in:
1.  **Identifying multi-scale frames** of visual evidence.
2.  **Reasoning over cross-frame clues** to connect information.
3.  **Deciding plausible actions** based on its deductions.

**ğŸ† Performance Highlights**
Here's a glimpse of Conan's impressive capabilities:
<img src="./figure/teaser.png"/>

## âš™ï¸ Environment Setup

- Clone the Repository:

```
git clone https://github.com/OuyangKun10/Conan.git
cd Conan
```

- Create and Activate Environment:
```
conda create --name Conan python=3.10
conda activate Conan
```
- Install Dependencies:
```
cd ms-swift
```
```
pip install -e .
```

## ğŸ‹ï¸ Training
```
cd ms-swift/training_scripts
```
1. **Texual Reasoning**
```
bash Conan-SFT-Stage1.sh
```
2. **Multimodal alignment Reasoning**
```
bash Conan-SFT-Stage2.sh
```
3. **Vision-centric Reasoning**
```
bash Conan-SFT-Stage3.sh
```
4. **AIR RLVR**
```
bash Conan-server.sh
bash Conan-AIR-RLVR.sh
```

## ğŸ“Š Evaluation

**Conan-Eval** toolkit allows for comprehensive evaluation across various benchmarks:

1.  **Multi-step Reasoning Benchmarks:**
    *   [MMR-V](https://mmr-v.github.io/home_page.html)
    *   [Video-Holmes](https://video-holmes.github.io/Page.github.io/)
    *   [VRBench](https://vrbench.github.io)
    *   [VCRBench](https://vlm-reasoning.github.io/VCR-Bench/)
    *   [LongVideoReason](https://huggingface.co/LongVideo-Reason)
    *   [HumanPCR](https://huggingface.co/datasets/HumanPCR/HumanPCR)

2.  **Long-video Understanding Benchmarks:**
    *   [LongVideoBench](https://longvideobench.github.io)
    *   [MLVU](https://github.com/JUNJIE99/MLVU)
    *   [LVBench](https://lvbench.github.io)
    *   [Video-MME](https://video-mme.github.io/home_page.html)

3.  **Usage:**
    To run the evaluation, simply execute:
    ```bash
    bash run.sh
    ```

## Acknowledgments
We extend our sincere gratitude to the following projects for their invaluable contributions and inspiration:
*   [ms-swift](https://github.com/modelscope/ms-swift)
*   [Video-R1](https://github.com/tulerfeng/Video-R1)
*   [Generative-sampler](https://generative-sampler.github.io)

